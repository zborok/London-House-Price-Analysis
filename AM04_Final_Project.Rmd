---
title: 'Session 10: Data Science Capstone Project'
author: "Zervaan Borok"
date: "`r Sys.Date()`"
output: 
    html_document:
      number_sections: true
      highlight: haddock
      theme: spacelab
      toc: yes
      toc_depth: 2
      toc_float:
        collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<style>
div.navy1 { background-color:#686868; border-radius: 5px; padding: 20px; border-style: groove; color: #ffffff;}

</style>



```{r, load_libraries, include = FALSE}

if(!is.element("tidyverse", installed.packages()[,1]))
{  install.packages("tidyverse")}

if(!is.element("Hmisc", installed.packages()[,1]))
{  install.packages("Hmisc")} #package for data summary using `describe`

if(!is.element("ggplot2", installed.packages()[,1]))
{  install.packages("ggplot2")} #package for plots
if(!is.element("ggthemes", installed.packages()[,1]))
{  install.packages("ggthemes")} #package to make fancier ggplots

if(!is.element("janitor", installed.packages()[,1]))
{ install.packages("janitor")} #package to visualize results of machine learning tools
if(!is.element("rpart.plot", installed.packages()[,1]))
{  install.packages("rpart.plot")} #package to visualize trees

library(rpart.plot)
library(caret)
library(tidyverse) # the usual stuff: dplyr, readr, and other goodies
library(lubridate)
library(janitor) # clean_names()
library(Hmisc)
library(Metrics)
library(caretEnsemble)
library(doMC)
registerDoMC(8)
```

# Introduction and learning objectives

<div class = "navy1">
The purpose of this exercise is to build an estimation engine to guide investment decisions in London house market. You will first build machine learning algorithms (and tune them) to estimate the house prices given variety of information about each property. Then, using your algorithm, you will choose 200 houses to invest in out of about 2000 houses on the market at the moment.


<b>Learning objectives</b>
 
<ol type="i">
  <li>Using different data mining algorithms for prediction.</li>
  <li>Dealing with large data sets</li>
  <li>Tuning data mining algorithms</li>
  <li>Interpreting data mining algorithms and deducing importance of variables</li>
  <li>Using results of data mining algorithms to make business decisions</li>
</ol>  
</div>

# Load data

**There are two sets of data, i) training data that has the actual prices ii) out of sample data that has the asking prices. Load both data sets.** 


```{r message=FALSE, warning=FALSE}
# Read in the data
london_house_prices_2019_training <- read.csv("C:/Users/zerva/OneDrive/Documents/R Scipts LBS/am10.mam2022/training_data_assignment_with_prices.csv")
london_house_prices_2019_out_of_sample <- read.csv("C:/Users/zerva/OneDrive/Documents/R Scipts LBS/am10.mam2022/test_data_assignment.csv")


# Fix data types in both data sets

# Fix dates
london_house_prices_2019_training <- london_house_prices_2019_training %>% mutate(date=as.Date(date))
london_house_prices_2019_out_of_sample <- london_house_prices_2019_out_of_sample %>% mutate(date=as.Date(date))

# Change characters to factors
london_house_prices_2019_training <- london_house_prices_2019_training %>% mutate_if(is.character,as.factor)
london_house_prices_2019_out_of_sample <- london_house_prices_2019_out_of_sample %>% mutate_if(is.character,as.factor)

# Take a quick look at what's in the data
str(london_house_prices_2019_training)
str(london_house_prices_2019_out_of_sample)
```


```{r split the price data to training and testing, message=FALSE, warning=FALSE}
# Let's do the initial split
library(rsample)
set.seed(100)

train_test_split <- initial_split(london_house_prices_2019_training, prop = 0.75) #training set contains 75% of the data

# Create the training dataset
train_data <- training(train_test_split)
test_data <- testing(train_test_split)
```


# Visualize data 

**Visualize and examine the data. What plots could be useful here? What do you learn from these visualizations?**

Boxplots, histograms, and density plots will likely be the most useful for this data set. From the visualizations below, we can see that all of the variables inspected show exigent skewness. Thus, the data should be centered and scaled before being passed to any algorithms. For linear models, we can instead try regressing our predictor variables on the log of the price.

```{r message=FALSE, warning=FALSE}
# Prevent R from displaying plot tick marks in scientific notation
options(scipen = 100)
```


```{r message=FALSE, warning=FALSE}
# Table of number of properties by London Zone
library(DT)
zones_df <- as.data.frame(table(london_house_prices_2019_training$london_zone))
names(zones_df)[names(zones_df) == "Var1"] <- "London Zone"
names(zones_df)[names(zones_df) == "Freq"] <- "Number of Properties"
zones_df <- datatable(zones_df, width = 350) %>% formatStyle('London Zone', target = 'row', backgroundColor = 
                                       styleEqual(c(1,2,3,4,5,6,7), c('lightgray', 'white', 'lightgray', 'white', 'lightgray', 'white', 'lightgray')))
zones_df
```


```{r fig1, fig.width = 16, fig.height = 6, fig.align = 'center', message=FALSE, warning=FALSE}
# Distribution of Number of Properties by London Zone
ggplot(data = london_house_prices_2019_training, aes(x=london_zone)) +
  geom_histogram(binwidth = 0.5, fill = 'darkblue') +
    labs(title = "Distribution of Number of Properties by London Zone",
                      x = "London Zone", y = "Frequency") +
  theme(axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0))) +
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 12, b = 0, l = 0))) +
  theme(plot.title = element_text(margin = margin(t = 0, r = 0, b = 16, l = 0))) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(plot.title = element_text(face = "bold")) +
  theme(plot.title = element_text(size = 17)) +
  theme(axis.title.x = element_text(face = "bold")) +
  theme(axis.title.x = element_text(size = 12)) +
  theme(axis.title.y = element_text(face = "bold")) +
  theme(axis.title.y = element_text(size = 12)) +
  scale_x_continuous(breaks=seq(0,7,1), limits = c(0.75,7.5)) +
  scale_y_continuous(breaks=seq(0,4000,500),limits = c(0,4000), labels = scales::comma) +
  stat_bin(aes(y=..count.. , label=..count..), geom="text", binwidth=1, vjust = -1, fontface='bold', size = 4.25)
```


```{r fig2, fig.width = 16, fig.height = 6, fig.align = 'center', message=FALSE, warning=FALSE}
library(Hmisc)

# Boxplot of House Prices by London Zone
BXplot <- boxplot(price ~ london_zone, data = london_house_prices_2019_training, horizontal = TRUE, ylim = c(0, 11000000), ylab = 'London Zone', xlab = 'Price',
                  main = 'Boxplot of London House Prices', las = 1)
minor.tick(nx = 2, ny = 0, tick.ratio = 0.75)
```


```{r message=FALSE, warning=FALSE}
# Boxplot summary statistics
library(DT)

BXplot_stats <- datatable(BXplot$stats, rownames = c("Whisker Minimum", "25th Percentile", "Median", "75th Percentile", "Whisker Maximum"), 
                          colnames = c("Zone 1" = 'V1', "Zone 2" = 'V2', "Zone 3" = 'V3', "Zone 4" = 'V4', "Zone 5" = 'V5', "Zone 6" = 'V6', "Zone 7" = 'V7'))

BXplot_stats <- formatCurrency(BXplot_stats, columns = c("Zone 1", "Zone 2", "Zone 3", "Zone 4", "Zone 5", "Zone 6", "Zone 7"), currency = "£", 
                               interval = 3, mark = ",", digits = 2, before = TRUE)

BXplot_stats
```


```{r fig3, fig.width = 16, fig.height = 6, fig.align = 'center', message=FALSE, warning=FALSE}
# Price density by London Zone
ggplot(data = london_house_prices_2019_training, aes(x = price)) +
  geom_density() +
  labs(title = "Density of House Prices by London Zone",
                      x = "Price (£)", y = "Density") +
  theme(axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0))) +
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 12, b = 0, l = 0))) +
  theme(plot.title = element_text(margin = margin(t = 0, r = 0, b = 16, l = 0))) +
  facet_wrap(london_house_prices_2019_training$london_zone) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(plot.title = element_text(face = "bold")) +
  theme(plot.title = element_text(size = 17)) +
  theme(axis.title.x = element_text(face = "bold")) +
  theme(axis.title.x = element_text(size = 12)) +
  theme(axis.title.y = element_text(face = "bold")) +
  theme(axis.title.y = element_text(size = 12))
```


```{r fig4, fig.width = 16, fig.height = 6, fig.align = 'center', message=FALSE, warning=FALSE}
# Average price by Distance to Station
ggplot(data = london_house_prices_2019_training, aes(x = distance_to_station, y = mean(price))) +
  geom_col(color = 'darkorange') +
  labs(title = "Distribution of Average House Price by Distance to Tube Station",
                      x = "Distance to Station (km)", y = "Average Price (£)") +
  theme(axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0))) +
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 12, b = 0, l = 0))) +
  theme(plot.title = element_text(margin = margin(t = 0, r = 0, b = 16, l = 0))) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(plot.title = element_text(face = "bold")) +
  theme(plot.title = element_text(size = 17)) +
  theme(axis.title.x = element_text(face = "bold")) +
  theme(axis.title.x = element_text(size = 12)) +
  theme(axis.title.y = element_text(face = "bold")) +
  theme(axis.title.y = element_text(size = 12) )+
  scale_x_continuous(breaks=seq(0,4,0.25), limits = c(0,3.25)) +
  scale_y_continuous(breaks=seq(0,16000000,3000000), labels = scales::comma, limits = c(0,16000000))
```


```{r message=FALSE, warning=FALSE}
# Table of number of properties by number of habitable rooms
rooms_df <- as.data.frame(table(london_house_prices_2019_training$number_habitable_rooms))
names(rooms_df)[names(rooms_df) == "Var1"] <- "Number of Habitable Rooms"
names(rooms_df)[names(rooms_df) == "Freq"] <- "Number of Properties"
rooms_df <- datatable(rooms_df, height = 800, width = 350) %>% formatStyle('Number of Habitable Rooms', target = 'row', backgroundColor = 
                                       styleEqual(c(1,2,3,4,5,6,7,8,9,10,11,12,13,14), c('lightgray', 'white', 'lightgray', 'white', 'lightgray', 'white', 'lightgray', 'white', 'lightgray', 'white', 'lightgray', 'white', 'lightgray', 'white')))
rooms_df
```


```{r fig5, fig.width = 16, fig.height = 6, fig.align = 'center', message=FALSE, warning=FALSE}
# Distribution of Number of Habitable Rooms
ggplot(data = london_house_prices_2019_training, aes(x=number_habitable_rooms)) +
  geom_histogram(binwidth = 0.5, fill = 'darkblue') +
    labs(title = "Distribution of Number of Habitable Rooms",
                      x = "Number of Habitable Rooms", y = "Frequency") +
  theme(axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0))) +
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 12, b = 0, l = 0))) +
  theme(plot.title = element_text(margin = margin(t = 0, r = 0, b = 16, l = 0))) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(plot.title = element_text(face = "bold")) +
  theme(plot.title = element_text(size = 17)) +
  theme(axis.title.x = element_text(face = "bold")) +
  theme(axis.title.x = element_text(size = 12)) +
  theme(axis.title.y = element_text(face = "bold")) +
  theme(axis.title.y = element_text(size = 12)) +
  scale_x_continuous(breaks=seq(0,14,1), limits = c(0.5,14)) +
  scale_y_continuous(breaks=seq(0,4000,500),limits = c(0,4000), labels = scales::comma) +
  stat_bin(aes(y=..count.. , label=..count..), geom="text", binwidth=1, vjust = -1, fontface='bold', size = 4.25)
```


```{r fig6, fig.width = 16, fig.height = 6, fig.align = 'center', message=FALSE, warning=FALSE}
# Distribution of Distance to Station
ggplot(data = london_house_prices_2019_training, aes(x=distance_to_station)) +
  geom_histogram(binwidth = 0.15, color = 'red') +
    labs(title = "Distribution of Distance to Station",
                      x = "Distance to Station (km)", y = "Frequency") +
  theme(axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0))) +
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 12, b = 0, l = 0))) +
  theme(plot.title = element_text(margin = margin(t = 0, r = 0, b = 16, l = 0))) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(plot.title = element_text(face = "bold")) +
  theme(plot.title = element_text(size = 17)) +
  theme(axis.title.x = element_text(face = "bold")) +
  theme(axis.title.x = element_text(size = 12)) +
  theme(axis.title.y = element_text(face = "bold")) +
  theme(axis.title.y = element_text(size = 12)) +
  scale_x_continuous(breaks=seq(0,4,0.15), limits = c(-0.1,3.25)) +
  scale_y_continuous(breaks=seq(0,3000,500),limits = c(0,3000), labels = scales::comma) +
  stat_bin(aes(y=..count.. , label=..count..), geom="text", binwidth=0.15, vjust = -1, fontface='bold', size = 4.25)
```


```{r fig7, fig.width = 16, fig.height = 6, fig.align = 'center', message=FALSE, warning=FALSE}
# Distribution of Floor Area
ggplot(data = london_house_prices_2019_training, aes(x = total_floor_area)) +
  geom_histogram(binwidth=15, color = 'green') +
    labs(title = "Distribution of Total Floor Area",
                      x = "Total Floor Area", y = "Frequency") +
  theme(axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0))) +
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 12, b = 0, l = 0))) +
  theme(plot.title = element_text(margin = margin(t = 0, r = 0, b = 16, l = 0))) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(plot.title = element_text(face = "bold")) +
  theme(plot.title = element_text(size = 17)) +
  theme(axis.title.x = element_text(face = "bold")) +
  theme(axis.title.x = element_text(size = 12)) +
  theme(axis.title.y = element_text(face = "bold")) +
  theme(axis.title.y = element_text(size = 12)) +
  scale_x_continuous(breaks=seq(0,500,25), limits = c(1,490)) +
  scale_y_continuous(breaks=seq(0,3000,500),limits = c(0,3000), labels = scales::comma) +
  stat_bin(aes(y=..count.. , label=..count..), geom="text", binwidth=15, vjust = -1, fontface='bold', size = 4.25)
```


**Estimate a correlation table between prices and other continuous variables. What do you glean from the correlation table?**

For the most part, the variables within this data set are mildly to moderately correlated. A few are strongly correlated, such as 'number_habitable_rooms' and 'total_floor_area', but there are no correlations with an absolute correlation coefficient magnitude greater than 0.85. 

```{r correlation table, warning=FALSE, message=FALSE}
library("GGally")

# Correlation Plot
london_house_prices_2019_training %>% 
  select(-ID) %>% #keep Y variable last
  ggcorr(method = c("pairwise", "pearson"), layout.exp = 6,label_round=2, label = TRUE,label_size = 2.5,hjust = 1,nbreaks = 6,angle = -8, size = 3)
```


```{r message=FALSE, warning=FALSE}
# Remove unnecessary columns for Lasso
train_data_2 <- train_data[c(-1,-2,-3,-7,-8,-9,-10)]
train_data_2 <- na.omit(train_data_2)
```


```{r message=FALSE, warning=FALSE}
# Assign response and explanatory variables
y <- log(train_data_2$price)

x <- data.matrix(train_data_2[, c('property_type', 'whether_old_or_new', 'freehold_or_leasehold', 'local_aut', 'county', 'postcode_short',
                                  'current_energy_rating', 'total_floor_area', 'number_habitable_rooms', 'co2_emissions_current', 'co2_emissions_potential', 'energy_consumption_current', 'energy_consumption_potential', 'windows_energy_eff', 'tenure', 'latitude', 'longitude', 'population', 'altitude', 'london_zone', 'nearest_station', 'water_company', 'average_income', 'district', 'type_of_closest_station', 'num_tube_lines', 'num_rail_lines', 'num_light_rail_lines', 'distance_to_station')])
```


```{r message=FALSE, warning=FALSE}
# Lasso regression for variable importance
library(glmnet)

# Perform k-fold cross-validation to find optimal lambda value
lasso_model <- cv.glmnet(x, y, alpha = 1)

# Find optimal lambda value that minimizes test MSE
best_lambda <- lasso_model$lambda.min
best_lambda

plot(lasso_model)
```


```{r message=FALSE, warning=FALSE}
# Find coefficients of best model
best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(best_model)
```


```{r message=FALSE, warning=FALSE}
# Use fitted best model to make predictions
y_predicted <- predict(best_model, s = best_lambda, newx = x)
y_predicted <- y_predicted

# Find SST and SSE
sst <- sum((y - mean(y))^2)
sse <- sum((y_predicted - y)^2)

# Find R-Squared
rsq <- 1 - sse/sst
rsq
```


# Fit a linear regression model

```{r message=FALSE, warning=FALSE}
# Define control variables
control <- trainControl (
    method="cv",
    number=10,
    verboseIter=TRUE) #by setting this to true the model will report its progress after each estimation

# We are going to train the model and report the results using k-fold cross validation
model1_lm<-train(
    log(price) ~ distance_to_station + property_type + longitude + total_floor_area + number_habitable_rooms 
       + london_zone + type_of_closest_station + num_tube_lines + num_light_rail_lines + average_income + district + windows_energy_eff + co2_emissions_potential + co2_emissions_current + energy_consumption_potential,
    train_data,
   method = "lm",
    trControl = control
   )

# Summary of the results
summary(model1_lm)
```


```{r fig.height=14, fig.width=10, fig.align = 'center', message=FALSE, warning=FALSE}
# Check variable importance as well
importance_lm <- varImp(model1_lm, scale=TRUE)
plot(importance_lm)
```


```{r message=FALSE, warning=FALSE}
# List definitions of variables
df_vars <- data.frame(Variable  = c("distance to station", "property type", "longitude", "total floor area", "number habitable rooms", "london zone", "type of closest station", "num tube lines", "num light rail lines", "average income", "district", "windows energy eff", "co2 emissions potential", "co2 emissions current", "energy consumption potential"),
                  Definition = c("The distance in kilometers to the nearest station from the postcode", "D = Detached, S = Semi-Detached, T = Terraced, F = Flats/Maisonettes, O = Other", "Longitude of centroid of the Postcode in decimal format", "Total of all enclosed spaces measured to the internal face of the external walls in square meters", "Habitable rooms include any living room, sitting room, dining room, bedroom, study and similar", "Transport for London (TFL) Travel Zone Indicator", "The type of closest train station (i.e. tube, DLR, or overground)", "Number of tube lines that use the closest station", "Number of light rail lines that use the closest station", "Average household income of the MSOA that the postcode is located in", "The district the property is in (i.e. Kensington, Chelsea, etc)", "Energy efficiency rating of windows with values of very poor, poor, average, good, very good", "Estimated value in Tonnes per Year of the total CO2 emissions produced by the property in 12 month period", "CO2 emissions per year in tonnes/year", "Estimated potential total energy consumption for the property in a 12 month in kilowatt hours per square meter")
                  )
df_vars
```


## Predict the values in testing and out of sample data

**Below I use the predict function to test the performance of the model in testing data and summarize the performance of the linear regression model. How can you measure the quality of your predictions?**

```{r message=FALSE, warning=FALSE}
# Predict the testing values
predictions <- predict(model1_lm,test_data)

lr_results<-data.frame(  RMSE = RMSE(exp(predictions), test_data$price), 
                            Rsquare = R2(exp(predictions), test_data$price))

                            
lr_results                         

# Predict prices for out of sample data the same way
predictions_oos <- predict(model1_lm,london_house_prices_2019_out_of_sample)

lr_pred<-data.frame(  RMSE = RMSE(exp(predictions_oos), london_house_prices_2019_out_of_sample$asking_price), 
                            Rsquare = R2(exp(predictions_oos), london_house_prices_2019_out_of_sample$asking_price))

lr_pred
```



# Fit a tree model

**Compare the performance of the linear regression model with the tree model; which one performs better? Why do you think that is the case?**

```{r fig.height=14, fig.width=10, fig.align = 'center', message=FALSE, warning=FALSE}
# Construct Tree Model
model2_tree <- train(
  price ~ distance_to_station + property_type + longitude + total_floor_area + number_habitable_rooms 
       + london_zone + type_of_closest_station + num_tube_lines + num_light_rail_lines + average_income + district + windows_energy_eff + co2_emissions_potential + co2_emissions_current + energy_consumption_potential,
  train_data,
  method = "rpart",
  trControl = control,
  tuneLength=35,
  preProcess = c("center", "scale")
    )

# View how the tree performs
model2_tree$results

# View the final tree
rpart.plot(model2_tree$finalModel)

# Visualize the variable importance
importance_tree <- varImp(model2_tree, scale=TRUE)
plot(importance_tree)
```


```{r message=FALSE, warning=FALSE}
# Predict the testing values
predictions_tree <- predict(model2_tree,test_data)

tree_results<-data.frame(  RMSE = RMSE(predictions_tree, test_data$price), 
                            Rsquare = R2(predictions_tree, test_data$price))

                            
tree_results                         

# Predict prices for out of sample data the same way
predictions_oos_tree <- predict(model2_tree,london_house_prices_2019_out_of_sample)

tree_pred<-data.frame(  RMSE = RMSE(predictions_oos_tree, london_house_prices_2019_out_of_sample$asking_price), 
                            Rsquare = R2(predictions_oos_tree, london_house_prices_2019_out_of_sample$asking_price))

tree_pred
```


# Other algorithms

**Use at least two other algorithms to predict prices. Don't forget to tune the parameters of these algorithms. And then compare the performances of your algorithms to linear regression and trees.**

```{r message=FALSE, warning=FALSE}
set.seed(100)
# KNN Model
knn_fit <- train(log(price) ~ distance_to_station + property_type + longitude + total_floor_area + number_habitable_rooms 
       + london_zone + type_of_closest_station + num_tube_lines + num_light_rail_lines + average_income + district + windows_energy_eff + co2_emissions_potential + co2_emissions_current + energy_consumption_potential, data= train_data, 
                 method = "knn",
                 trControl = trainControl("cv", number = 10), #use cross validation with 10 data points
                 tuneLength = 10, #number of parameter values train function will try
                 preProcess = c("center", "scale"))  #center and scale the data 

knn_fit
```


```{r fig.height=6, fig.width=10, fig.align = 'center', message=FALSE, warning=FALSE}
# Check variable importance 
importance_knn <- varImp(knn_fit, scale=TRUE)
plot(importance_knn) 
```


```{r message=FALSE, warning=FALSE}
# Predict the testing values
predictions_knn <- predict(knn_fit,test_data)
predictions_knn <- exp(predictions_knn)

knn_results<-data.frame(  RMSE = RMSE(predictions_knn, test_data$price), 
                            Rsquare = R2(predictions_knn, test_data$price))

knn_results                         

# Predict prices for out of sample data the same way
predictions_oos_knn <- predict(knn_fit,london_house_prices_2019_out_of_sample)
predictions_oos_knn <- exp(predictions_oos_knn)

knn_pred<-data.frame(  RMSE = RMSE(predictions_oos_knn, london_house_prices_2019_out_of_sample$asking_price), 
                            Rsquare = R2(predictions_oos_knn, london_house_prices_2019_out_of_sample$asking_price))

knn_pred
```

```{r message=FALSE, warning=FALSE}
# Optimal Value for k
optimal_k <- knn_fit$bestTune
optimal_k <- optimal_k[,1]
optimal_k
```


```{r message=FALSE, warning=FALSE}
set.seed(100)
# Random Forest Model
rf_fit <- train(log(price) ~ distance_to_station + property_type + longitude + total_floor_area + number_habitable_rooms 
       + london_zone + type_of_closest_station + num_tube_lines + num_light_rail_lines + average_income + district + windows_energy_eff + co2_emissions_potential + co2_emissions_current + energy_consumption_potential, data= train_data, 
                 method = "ranger", 
                 metric = "Rsquared",
                 trControl = control,
                 tuneLength = 10,
                # tuneGrid = grid,
                 preProcess = c("center", "scale"),
                 importance = 'permutation',
                 verbose = FALSE
                 )
```


```{r fig.height=14, fig.width=10, fig.align = 'center', message=FALSE, warning=FALSE}
# Check variable importance as well
importance_rf <- varImp(rf_fit, scale=TRUE)
plot(importance_rf)
```


```{r message=FALSE, warning=FALSE}
# Predict the testing values
predictions_rf <- predict(rf_fit,test_data)
predictions_rf <- exp(predictions_rf)

rf_results<-data.frame(  RMSE = RMSE(predictions_rf, test_data$price), 
                            Rsquare = R2(predictions_rf, test_data$price))

rf_results                         

# Predict prices for out of sample data the same way
predictions_oos_rf <- predict(rf_fit,london_house_prices_2019_out_of_sample)
predictions_oos_rf <- exp(predictions_oos_rf)

rf_pred<-data.frame(  RMSE = RMSE(predictions_oos_rf, london_house_prices_2019_out_of_sample$asking_price), 
                            Rsquare = R2(predictions_oos_rf, london_house_prices_2019_out_of_sample$asking_price))

rf_pred
```


```{r message=FALSE, warning=FALSE}
# Optimal Parameters for Random Forest
rf_params <- rf_fit$bestTune
rf_params

optimal_mtry <- rf_params[,1]
optimal_mim_node_size <- rf_params[,3]
```


```{r message=FALSE, warning=FALSE}
# Give definitions of parameters
df_rf_params <- data.frame(Parameter = c("mtry", "splitrule", "minimum.node.size"), Definition = c("Number of randomly chosen variables to do a split each time", "Purity measure", "Minimum size allowed for a leaf"))

df_rf_params
```


```{r message=FALSE, warning=FALSE}
set.seed(100)
# GBM Model
library(gbm)

grid <- expand.grid(interaction.depth = 6,n.trees = 150,shrinkage =0.075, n.minobsinnode = 10)

gbm_fit <- train(log(price) ~ distance_to_station + property_type + longitude + total_floor_area + number_habitable_rooms 
       + london_zone + type_of_closest_station + num_tube_lines + num_light_rail_lines + average_income + district + windows_energy_eff + co2_emissions_potential + co2_emissions_current + energy_consumption_potential, data= train_data, 
                 method = "gbm", 
                 trControl = control,
                 tuneLength = 35,
                 preProcess = c("center", "scale"),
                 tuneGrid = grid,
                 verbose = FALSE
                 )
```


```{r message=FALSE, warning=FALSE}
# Give definition of parameters
df_gbm_params <- data.frame(Parameter = c("n.trees", "interaction.depth", "shrinkage", "n.minobsinnode’"), Definition = c("Number of iterations", "Complexity of tree", "Learning rate, i.e. how quickly the algorithm adapts", "The minimum number of training set samples in a node to stop splitting"))

df_gbm_params
```


```{r fig.height=14, fig.width=10, fig.align = 'center', message=FALSE, warning=FALSE}
# Check variable importance 
importance_gbm <- varImp(gbm_fit, scale=TRUE)
plot(importance_gbm)
```


```{r message=FALSE, warning=FALSE}
# Predict the testing values
predictions_gbm <- predict(gbm_fit,test_data)
predictions_gbm <- exp(predictions_gbm)

gbm_results<-data.frame(  RMSE = RMSE(predictions_gbm, test_data$price), 
                            Rsquare = R2(predictions_gbm, test_data$price))

gbm_results                         

# Predict prices for out of sample data the same way
predictions_oos_gbm <- predict(gbm_fit,london_house_prices_2019_out_of_sample)
predictions_oos_gbm <- exp(predictions_oos_gbm)

gbm_pred<-data.frame(  RMSE = RMSE(predictions_oos_gbm, london_house_prices_2019_out_of_sample$asking_price), 
                            Rsquare = R2(predictions_oos_gbm, london_house_prices_2019_out_of_sample$asking_price))

gbm_pred
```


# Stacking

Use stacking to ensemble your algorithms.
```{r message=FALSE, warning=FALSE}
library(caret)
```


```{r warning=FALSE,  message=FALSE}
set.seed(100)
# Ensemble Model
  model_list_1 <- caretList(
    log(price) ~ distance_to_station + property_type + longitude + total_floor_area + number_habitable_rooms 
       + london_zone + type_of_closest_station + num_tube_lines + num_light_rail_lines + average_income + district + windows_energy_eff + co2_emissions_potential + co2_emissions_current + energy_consumption_potential, data = train_data,
    trControl=control,
    metric = "Rsquared",
    #methodList=c("glm"),
     tuneList=list(
            gbm=caretModelSpec(method="gbm", tuneGrid = data.frame(interaction.depth = 6,n.trees = 150,shrinkage = 0.075, n.minobsinnode = 10),verbose = FALSE, tuneLength = 35, 
                               preProcess = c("center", "scale")),
            knn=caretModelSpec(method="knn", tuneGrid=data.frame(k=optimal_k), tuneLength = 10, preProcess = c("center", "scale")),
            ranger=caretModelSpec(method="ranger", tuneGrid=data.frame(mtry=optimal_mtry,splitrule="extratrees",min.node.size=optimal_mim_node_size), tuneLength = 10, 
                                  preProcess = c("center", "scale")),
            #rpart=caretModelSpec(method="rpart",  tuneGrid=data.frame(cp=0.0011),minbucket=2, tuneLength = 35, preProcess = c("center", "scale"))
            lm=caretModelSpec(method = "lm", preProcess = c("center", "scale"))
           ))
```


```{r message=FALSE, warning=FALSE}
# Correlation of Models within Stack
summary(model_list_1)

set.seed(100)

modelCor(resamples(model_list_1))
```


```{r message=FALSE, warning=FALSE}
# 95% C.I. for Rsquared of Each Model within Stack
set.seed(100)

resamples <- resamples(model_list_1)

dotplot(resamples, metric = "Rsquared")
```


```{r message=FALSE, warning=FALSE}
glm_ensemble_1 <- caretStack(
    model_list_1,                   # Models we trained above in caretList 
    method="glm",                   # Use logistic regression to combine
    metric="Rsquared",              # Use Rsquared to measure quality of fit
    trControl=control
  )

summary(glm_ensemble_1) 
```


```{r message=FALSE, warning=FALSE}
# Predict the testing values
glm_test_pred_1 <- predict(glm_ensemble_1, test_data)
glm_test_pred_1 <- exp(glm_test_pred_1)

glm_results_1<-data.frame(  RMSE = RMSE(glm_test_pred_1, test_data$price), 
                            Rsquare = R2(glm_test_pred_1, test_data$price))

glm_results_1                         

# Predict prices for out of sample data the same way
predictions_oos_glm_1 <- predict(glm_ensemble_1,london_house_prices_2019_out_of_sample)
predictions_oos_glm_1 <- exp(predictions_oos_glm_1)

glm_pred_1<-data.frame(  RMSE = RMSE(predictions_oos_glm_1, london_house_prices_2019_out_of_sample$asking_price), 
                            Rsquare = R2(predictions_oos_glm_1, london_house_prices_2019_out_of_sample$asking_price))

glm_pred_1
```


```{r message=FALSE, warning=FALSE}
# Model Comparison and Selection
library(dplyr)

# Create dataframe of all model's RMSE and Rsquared values on testing data
model_comparison_df_test <- rbind(lr_results, tree_results, knn_results, rf_results, gbm_results, glm_results_1)
names(model_comparison_df_test)[names(model_comparison_df_test) == "RMSE"] <- "RMSE_Test"
names(model_comparison_df_test)[names(model_comparison_df_test) == "Rsquare"] <- "Rsquare_Test"
model_comparison_df_test <- model_comparison_df_test %>%
  mutate(Model = c("Linear Regression", "Decision Tree", "K Nearest Neighbor", "Random Forest", "Gradient Boosting Machine", "Stacked Model"), .before = RMSE_Test)

# Create dataframe of all model's RMSE and Rsquared values on out of sample data
model_comparison_df_oos <- rbind(lr_pred, tree_pred, knn_pred, rf_pred, gbm_pred, glm_pred_1)
names(model_comparison_df_oos)[names(model_comparison_df_oos) == "RMSE"] <- "RMSE_OOS"
names(model_comparison_df_oos)[names(model_comparison_df_oos) == "Rsquare"] <- "Rsquare_OOS"
model_comparison_df_oos <- model_comparison_df_oos %>%
  mutate(Model = c("Linear Regression", "Decision Tree", "K Nearest Neighbor", "Random Forest", "Gradient Boosting Machine", "Stacked Model"), .before = RMSE_OOS)

# Combine above dataframes and create average RMSE and Rsquared metrics for each model
combined_model_comparison <- left_join(model_comparison_df_test, model_comparison_df_oos, by = 'Model')
combined_model_comparison <- combined_model_comparison %>%
  mutate(Average_RMSE = (RMSE_Test + RMSE_OOS)/2) %>%
  mutate(Average_Rsquare = (Rsquare_Test + Rsquare_OOS)/2)

combined_model_comparison 

# Select model with lowest average RMSE and highest average Rsquared
optimal_model <- combined_model_comparison[which(combined_model_comparison$Average_RMSE == min(combined_model_comparison$Average_RMSE) | combined_model_comparison$Average_Rsquare == max(combined_model_comparison$Average_Rsquare)),]

optimal_model
```



# Pick investments

In this section you should use the best algorithm you identified to choose 200 properties from the out of sample data.


```{r message=FALSE, warning=FALSE}
# Add predicted price column to OOS dataframe
out_of_sample_df <- london_house_prices_2019_out_of_sample

out_of_sample_df <- out_of_sample_df %>%
  mutate(predicted_price = predictions_oos_glm_1)
```


```{r message=FALSE, warning=FALSE}
# Add percentage price column to OOS dataframe
out_of_sample_df <- out_of_sample_df %>%
  mutate(percent_profit = ((predicted_price - asking_price)/asking_price)*100)
```


```{r message=FALSE, warning=FALSE}
# Re-order dataframe by percentage profit column
out_of_sample_df <- out_of_sample_df[order(-out_of_sample_df$percent_profit),]
```


```{r message=FALSE, warning=FALSE}
# Calculate Total Raw Profit from Chosen Investments
out_of_sample_df <- out_of_sample_df %>%
  mutate(raw_profit = predicted_price - asking_price)
```


```{r message=FALSE, warning=FALSE}
# Convert Total Raw Profit to Currency
total_raw_acq_cost <- as.data.frame(sum(out_of_sample_df$asking_price[1:200]))
names(total_raw_acq_cost)[names(total_raw_acq_cost) == "sum(out_of_sample_df$asking_price[1:200])"] <- "Price Value"

total_raw_revenue <- as.data.frame(sum(out_of_sample_df$predicted_price[1:200]))
names(total_raw_revenue)[names(total_raw_revenue) == "sum(out_of_sample_df$predicted_price[1:200])"] <- "Price Value"

total_raw_profit <- as.data.frame(sum(out_of_sample_df$raw_profit[1:200]))
names(total_raw_profit)[names(total_raw_profit) == "sum(out_of_sample_df$raw_profit[1:200])"] <- "Price Value"

acq_rev_profit <- rbind(total_raw_acq_cost, total_raw_revenue, total_raw_profit)

acq_rev_profit <- datatable(acq_rev_profit, rownames = c("Total Acquisition Cost", "Total Revenue", "Total Raw Profit"))
acq_rev_profit <- formatCurrency(acq_rev_profit, columns = "Price Value", currency = "£", interval = 3, mark = ",", digits = 2, before = TRUE)
acq_rev_profit
```


```{r message=FALSE, warning=FALSE}
# Calculate Average Percent Profit from Chosen Investments
average_percent_profit <- mean(out_of_sample_df$percent_profit[1:200])
cat(average_percent_profit,"%")
```


```{r message=FALSE, warning=FALSE}
# Create Raw Profit and Percent Profit Dataframes from Chosen Investments
inv_choices <- out_of_sample_df[1:200, ]

zone_1 <- inv_choices[inv_choices$london_zone == 1,]
zone_2 <- inv_choices[inv_choices$london_zone == 2,]
zone_3 <- inv_choices[inv_choices$london_zone == 3,]
zone_4 <- inv_choices[inv_choices$london_zone == 4,]
zone_5 <- inv_choices[inv_choices$london_zone == 5,]
zone_6 <- inv_choices[inv_choices$london_zone == 6,]

zone_1_raw_avg <- as.data.frame(mean(zone_1$raw_profit))
zone_2_raw_avg <- as.data.frame(mean(zone_2$raw_profit))
zone_3_raw_avg <- as.data.frame(mean(zone_3$raw_profit))
zone_4_raw_avg <- as.data.frame(mean(zone_4$raw_profit))
zone_5_raw_avg <- as.data.frame(mean(zone_5$raw_profit))
zone_6_raw_avg <- as.data.frame(mean(zone_6$raw_profit))

names(zone_1_raw_avg)[names(zone_1_raw_avg) == "mean(zone_1$raw_profit)"] <- "Raw_Profit"
names(zone_2_raw_avg)[names(zone_2_raw_avg) == "mean(zone_2$raw_profit)"] <- "Raw_Profit"
names(zone_3_raw_avg)[names(zone_3_raw_avg) == "mean(zone_3$raw_profit)"] <- "Raw_Profit"
names(zone_4_raw_avg)[names(zone_4_raw_avg) == "mean(zone_4$raw_profit)"] <- "Raw_Profit"
names(zone_5_raw_avg)[names(zone_5_raw_avg) == "mean(zone_5$raw_profit)"] <- "Raw_Profit"
names(zone_6_raw_avg)[names(zone_6_raw_avg) == "mean(zone_6$raw_profit)"] <- "Raw_Profit"

zone_1_raw_avg <- zone_1_raw_avg %>%
  mutate(Zone = "Zone 1", .before = Raw_Profit)
zone_2_raw_avg <- zone_2_raw_avg %>%
  mutate(Zone = "Zone 2", .before = Raw_Profit)
zone_3_raw_avg <- zone_3_raw_avg %>%
  mutate(Zone = "Zone 3", .before = Raw_Profit)
zone_4_raw_avg <- zone_4_raw_avg %>%
  mutate(Zone = "Zone 4", .before = Raw_Profit)
zone_5_raw_avg <- zone_5_raw_avg %>%
  mutate(Zone = "Zone 5", .before = Raw_Profit)
zone_6_raw_avg <- zone_6_raw_avg %>%
  mutate(Zone = "Zone 6", .before = Raw_Profit)

combined_zones_1 <- rbind(zone_1_raw_avg, zone_2_raw_avg, zone_3_raw_avg, zone_4_raw_avg, zone_5_raw_avg, zone_6_raw_avg)



zone_1_perc_avg <- as.data.frame(mean(zone_1$percent_profit))
zone_2_perc_avg <- as.data.frame(mean(zone_2$percent_profit))
zone_3_perc_avg <- as.data.frame(mean(zone_3$percent_profit))
zone_4_perc_avg <- as.data.frame(mean(zone_4$percent_profit))
zone_5_perc_avg <- as.data.frame(mean(zone_5$percent_profit))
zone_6_perc_avg <- as.data.frame(mean(zone_6$percent_profit))

names(zone_1_perc_avg)[names(zone_1_perc_avg) == "mean(zone_1$percent_profit)"] <- "Percent_Profit"
names(zone_2_perc_avg)[names(zone_2_perc_avg) == "mean(zone_2$percent_profit)"] <- "Percent_Profit"
names(zone_3_perc_avg)[names(zone_3_perc_avg) == "mean(zone_3$percent_profit)"] <- "Percent_Profit"
names(zone_4_perc_avg)[names(zone_4_perc_avg) == "mean(zone_4$percent_profit)"] <- "Percent_Profit"
names(zone_5_perc_avg)[names(zone_5_perc_avg) == "mean(zone_5$percent_profit)"] <- "Percent_Profit"
names(zone_6_perc_avg)[names(zone_6_perc_avg) == "mean(zone_6$percent_profit)"] <- "Percent_Profit"

zone_1_perc_avg <- zone_1_perc_avg %>%
  mutate(Zone = "Zone 1", .before = Percent_Profit)
zone_2_perc_avg <- zone_2_perc_avg %>%
  mutate(Zone = "Zone 2", .before = Percent_Profit)
zone_3_perc_avg <- zone_3_perc_avg %>%
  mutate(Zone = "Zone 3", .before = Percent_Profit)
zone_4_perc_avg <- zone_4_perc_avg %>%
  mutate(Zone = "Zone 4", .before = Percent_Profit)
zone_5_perc_avg <- zone_5_perc_avg %>%
  mutate(Zone = "Zone 5", .before = Percent_Profit)
zone_6_perc_avg <- zone_6_perc_avg %>%
  mutate(Zone = "Zone 6", .before = Percent_Profit)

combined_zones_2 <- rbind(zone_1_perc_avg, zone_2_perc_avg, zone_3_perc_avg, zone_4_perc_avg, zone_5_perc_avg, zone_6_perc_avg)
```


```{r fig.height=6, fig.width=14, fig.align = 'center', message=FALSE, warning=FALSE}
# Plot of Predicted Price vs Asking Price
ggplot(out_of_sample_df, aes(x = predicted_price, y = asking_price)) +
  geom_point() +
  geom_abline(intercept=0, slope=1) +
  labs(x='Predicted Price (£)', y='Asking Price (£)', title='Predicted Price vs. Asking Price') +
  theme(axis.title.x = element_text(margin = margin(t = 14, r = 0, b = 0, l = 0))) +
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0))) +
  theme(plot.title = element_text(margin = margin(t = 0, r = 0, b = 16, l = 0))) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(plot.title = element_text(face = "bold")) +
  theme(plot.title = element_text(size = 17)) +
  theme(axis.title.x = element_text(face = "bold")) +
  theme(axis.title.x = element_text(size = 12)) +
  theme(axis.title.y = element_text(face = "bold")) +
  theme(axis.title.y = element_text(size = 12)) +
  scale_x_continuous(breaks=seq(0,10000000,1000000), limits = c(0,10000000), labels = scales::comma) +
  scale_y_continuous(breaks=seq(0,15000000,3000000),limits = c(0,17000000), labels = scales::comma)
```


```{r fig.height=6, fig.width=14, fig.align = 'center', message=FALSE, warning=FALSE}
library(scales)
# Plot of Average Raw Profits by London Zone
ggplot(combined_zones_1, aes(x = Zone, y = Raw_Profit)) +
  geom_bar(stat = 'identity', fill = 'darkgreen') +
    labs(title = "Average Raw Profit by London Zone",
                      x = "London Zone", y = " Average Raw Profit (£)") +
  theme(axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0))) +
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 12, b = 0, l = 0))) +
  theme(plot.title = element_text(margin = margin(t = 0, r = 0, b = 16, l = 0))) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(plot.title = element_text(face = "bold")) +
  theme(plot.title = element_text(size = 17)) +
  theme(axis.title.x = element_text(face = "bold")) +
  theme(axis.title.x = element_text(size = 12)) +
  theme(axis.title.y = element_text(face = "bold")) +
  theme(axis.title.y = element_text(size = 12)) +
  scale_y_continuous(breaks=seq(0,800000,100000),limits = c(0,800000), labels = scales::comma) +
  geom_text(aes(label = paste0("£",comma(stat(y)))), stat = 'summary', fun = sum, vjust = -1, fontface='bold', size = 4.25)
```


```{r fig.height=6, fig.width=14, fig.align = 'center', message=FALSE, warning=FALSE}
# Plot of Average Percent Profits by London Zone
ggplot(combined_zones_2, aes(x = Zone, y = Percent_Profit)) +
  geom_bar(stat = 'identity', fill = 'darkgreen') +
    labs(title = "Average Percent Profit by London Zone",
                      x = "London Zone", y = "Percent Profit") +
  theme(axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0))) +
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 12, b = 0, l = 0))) +
  theme(plot.title = element_text(margin = margin(t = 0, r = 0, b = 16, l = 0))) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(plot.title = element_text(face = "bold")) +
  theme(plot.title = element_text(size = 17)) +
  theme(axis.title.x = element_text(face = "bold")) +
  theme(axis.title.x = element_text(size = 12)) +
  theme(axis.title.y = element_text(face = "bold")) +
  theme(axis.title.y = element_text(size = 12)) +
  scale_y_continuous(breaks=seq(0,100,10),limits = c(0,100)) +
  geom_text(aes(label = paste0(round(stat(y), digits = 2),"%")), stat = 'summary', fun = sum, vjust = -1, fontface='bold', size = 4.25) 
```


```{r message=FALSE, warning=FALSE}
# Create column to indicate selected investment properties
out_of_sample_df <- out_of_sample_df %>%
  mutate(buy = NA)

out_of_sample_df$buy[1:200] <- 1
out_of_sample_df$buy[201:1999] <- 0
```


```{r message=FALSE, warning=FALSE}
# Export final dataframe to CSV file
write.csv(out_of_sample_df, "C://Users//zerva//OneDrive//Documents//R Scipts LBS//am10.mam2022//zervaan_borok_AM04_project_2.csv")
```